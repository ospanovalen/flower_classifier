# Flower Classifier

**Автор:** Ален Оспанов

## Постановка задачи

Проект посвящен разработке модели для классификации цветов по изображениям с использованием глубокого обучения. Задача состоит в том, чтобы по входному изображению цветка определить его класс с высокой точностью.

Цель проекта — построить точный классификатор, который сможет различать различные виды цветов на основе их визуальных признаков.

## Данные

**Источник:** [Kaggle Flowers Dataset](https://www.kaggle.com/datasets/rahmasleam/flowers-dataset/data)

**Характеристики датасета:**
- Формат: изображения цветов в формате JPG
- Структура: изображения организованы в подпапки по классам
- Классовый дисбаланс: проверяется через анализ количества изображений в каждом классе
- Вариативность: изображения содержат различные ракурсы, освещение и фоны

**Разделение данных:**
- Обучение: 90%
- Валидация: 5%
- Тест: 5%
- Метод разделения: `random_split` с фиксированным `random_seed=2024`

## Формат входных и выходных данных

**Вход:**
- Изображения цветов в формате RGB
- Размер: 224x224 пикселей (после предобработки)
- Нормализация: ImageNet стандарт (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

**Выход:**
- Класс цветка в формате целочисленного индекса
- Вероятности классов после softmax-слоя

## Метрики

Для оценки качества модели используются следующие метрики:
- **Accuracy** - общая точность классификации
- **F1-score** - гармоническое среднее precision и recall
- **Precision** - точность для каждого класса
- **Recall** - полнота для каждого класса

## Модели

### Базовая модель
Простое предсказание наиболее частого класса для установления нижней границы качества.

### Основная модель
- **Архитектура:** ReXNet-150 из библиотеки `timm` с предобученными весами
- **Дообучение:** Использование contrastive loss для улучшения качества эмбеддингов
- **Оптимизатор:** Adam
- **Устройство:** GPU (CUDA)
- **Валидация:** Проверка качества каждые 5 эпох с ранней остановкой при отсутствии улучшений

## Setup

### Требования
- Python >= 3.10, < 3.14
- Poetry для управления зависимостями
- CUDA-совместимая GPU (опционально, но рекомендуется)

### Установка зависимостей

1. Клонируйте репозиторий:
```bash
git clone <repository-url>
cd flower-classifier
```

2. Установите Poetry (если не установлен):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

3. Установите зависимости проекта:
```bash
poetry install
```

4. Установите зависимости для GPU (опционально):
```bash
poetry install --with gpu
```

5. Установите pre-commit хуки:
```bash
make pre-commit-install
```

6. Настройка DVC для работы с данными:
```bash
make dd  # Настройка удаленного хранилища DVC
```

## Data Management

Проект использует DVC (Data Version Control) для управления данными:

1. **Загрузка данных:**
```bash
make ddvc-dataset  # Загрузить датасет
```

2. **Загрузка моделей:**
```bash
make ddvc-model    # Загрузить предобученные модели
```

Данные и модели хранятся в удаленном S3-совместимом хранилище и не включены в git-репозиторий.

## Train

Для запуска обучения модели используйте:

```bash
make run-train
```

Конфигурация обучения управляется через Hydra и находится в папке `configs/`. Основные параметры:
- Размер батча
- Learning rate
- Количество эпох
- Параметры аугментации данных
- Настройки модели

Пример запуска с кастомными параметрами:
```bash
poetry run python -m flower_classifier.train --config-name=train batch_size=16 lr=0.001
```

## Production Preparation

### Экспорт в ONNX
Для подготовки модели к продакшену модель экспортируется в формат ONNX:

```bash
poetry run python -m flower_classifier.export_onnx --model-path=models/best_model.pth
```

### Экспорт в TensorRT
Дополнительная оптимизация модели для NVIDIA GPU:

```bash
poetry run python -m flower_classifier.export_tensorrt --onnx-path=models/model.onnx
```

### Артефакты поставки
После обучения и экспорта для развертывания требуются:
- Экспортированная модель (ONNX/TensorRT)
- Конфигурационные файлы
- Препроцессинг пайплайн
- Список классов и их маппинг

## Infer

### Локальный инференс
Для запуска предсказаний на новых данных:

```bash
make run-inference-pipeline
```

Или напрямую:
```bash
poetry run python -m flower_classifier.inference --image-path=path/to/image.jpg
```

### Формат входных данных для инференса
- Поддерживаемые форматы: JPG, PNG
- Рекомендуемое разрешение: от 224x224 пикселей
- Цветовое пространство: RGB

### Формат выходных данных
```json
{
  "predicted_class": "rose",
  "confidence": 0.95,
  "all_predictions": {
    "rose": 0.95,
    "tulip": 0.03,
    "daisy": 0.02
  }
}
```

## Inference Server

### MLflow Serving
Запуск сервера предсказаний через MLflow:

```bash
mlflow models serve -m models/mlflow_model -p 8080
```

### Triton Inference Server
Альтернативный вариант развертывания с Triton:

```bash
# Подготовка модели для Triton
poetry run python -m flower_classifier.prepare_triton

# Запуск Triton сервера
triton --model-repository=models/triton_models
```

## Logging

Проект использует MLflow для логирования экспериментов:
- Метрики обучения и валидации
- Гиперпараметры
- Артефакты модели
- Графики обучения

MLflow сервер доступен по адресу: `http://127.0.0.1:8080`

Графики и логи также сохраняются в папке `plots/`.

## Code Quality

Проект использует следующие инструменты для контроля качества кода:
- **black** - форматирование кода
- **isort** - сортировка импортов
- **flake8** - линтинг
- **pre-commit** - автоматическая проверка перед коммитом

Запуск проверки качества:
```bash
make format                # Форматирование кода
poetry run pre-commit run -a  # Запуск всех проверок
```

## Структура проекта

```
flower-classifier/
├── flower_classifier/      # Основной пакет
│   ├── __init__.py
│   ├── data/              # Модули работы с данными
│   ├── models/            # Архитектуры моделей
│   ├── training/          # Логика обучения
│   └── inference/         # Логика инференса
├── configs/               # Конфигурации Hydra
├── tests/                 # Тесты
├── data/                  # Данные (управляются DVC)
├── models/                # Сохраненные модели
├── plots/                 # Графики обучения
├── pyproject.toml         # Конфигурация зависимостей
├── Makefile              # Команды для разработки
└── README.md             # Этот файл
```

## Развертывание

Для развертывания в продакшене рекомендуется:
1. Использовать Docker-контейнер с моделью
2. Настроить API через FastAPI или Flask
3. Использовать MLflow Serving или Triton Inference Server
4. Настроить мониторинг и логирование запросов

## Лицензия

MIT License - см. файл LICENSE для деталей.
